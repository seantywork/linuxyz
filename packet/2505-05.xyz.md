#

```shell

# https://kubernetes.io/docs/reference/networking/ports-and-protocols/

```

#

```shell

root@node-0:~# kubectl get nodes
NAME     STATUS   ROLES           AGE   VERSION
node-0   Ready    control-plane   73s   v1.33.1

```

#

```shell

root@node-0:~# kubeadm token create --print-join-command 
kubeadm join 10.168.0.2:6443 --token r61w3k.oom2m7zqt6m8p0fc --discovery-token-ca-cert-hash sha256:9dcf53ebff2089c12cf3af75e4540e58674ccd44282ba0285420852e4ebc5114 
```
#

```shell

root@node-1:~# kubeadm join 10.168.0.2:6443 --token r61w3k.oom2m7zqt6m8p0fc --discovery-token-ca-cert-hash sha256:9dcf53ebff2089c12cf3af75e4540e58674ccd44282ba0285420852e4ebc5114 
[preflight] Running pre-flight checks
[preflight] Reading configuration from the "kubeadm-config" ConfigMap in namespace "kube-system"...
[preflight] Use 'kubeadm init phase upload-config --config your-config-file' to re-upload it.
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
[kubelet-check] The kubelet is healthy after 1.002236822s
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
```
#

```shell

root@node-0:~# kubectl get nodes
NAME                                             STATUS   ROLES           AGE     VERSION
node-0                                           Ready    control-plane   8m21s   v1.33.1
node-1.us-east4-b.c.vpn-server-422904.internal   Ready    <none>          47s     v1.33.1

```

#

```shell
root@node-2:~# kubeadm join 10.168.0.2:6443 --token r61w3k.oom2m7zqt6m8p0fc --discovery-token-ca-cert-hash sha256:9dcf53ebff2089c12cf3af75e4540e58674ccd44282ba0285420852e4ebc5114
[preflight] Running pre-flight checks
[preflight] Reading configuration from the "kubeadm-config" ConfigMap in namespace "kube-system"...
[preflight] Use 'kubeadm init phase upload-config --config your-config-file' to re-upload it.
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
[kubelet-check] The kubelet is healthy after 1.003909562s
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

```

#

```shell

root@node-0:~# kubectl get nodes
NAME                                             STATUS   ROLES           AGE    VERSION
node-0                                           Ready    control-plane   12m    v1.33.1
node-1.us-east4-b.c.vpn-server-422904.internal   Ready    <none>          5m3s   v1.33.1
node-2.us-east4-b.c.vpn-server-422904.internal   Ready    <none>          51s    v1.33.1
```


#

```shell

root@node-0:~# kubectl label node node-1.us-east4-b.c.vpn-server-422904.internal nodelabel=node-wrk-1 
node/node-1.us-east4-b.c.vpn-server-422904.internal labeled
root@node-0:~# kubectl label node node-2.us-east4-b.c.vpn-server-422904.internal nodelabel=node-wrk-2
node/node-2.us-east4-b.c.vpn-server-422904.internal labeled
```

#

```shell

root@node-0:~# kubectl create namespace wrk-1
namespace/wrk-1 created
root@node-0:~# kubectl create namespace wrk-2
namespace/wrk-2 created
root@node-0:~# vim 1.yaml
root@node-0:~# kubectl -n wrk-1 apply -f ./1.yaml 
service/node-wrk-1-ubuntu24 created
deployment.apps/node-wrk-1-ubuntu24 created
```

#

```shell

root@node-0:~# kubectl -n wrk-1 get pods 
NAME                                   READY   STATUS    RESTARTS   AGE
node-wrk-1-ubuntu24-684f7d8fd6-2zncq   1/1     Running   0          112s
```

#

```shell

root@node-0:~# kubectl -n wrk-2 apply -f ./2.yaml 
service/node-wrk-2-ubuntu24 created
deployment.apps/node-wrk-2-ubuntu24 created
```

#


```shell

root@node-0:~# kubectl -n wrk-2 get pods 
NAME                                   READY   STATUS    RESTARTS   AGE
node-wrk-2-ubuntu24-85748464f7-mwmrt   1/1     Running   0          3m5s

```


#

```shell


root@node-1:~# apt update && apt install -y tshark
```
#
```shell


root@node-2:~# apt update && apt install -y tshark
```

#

```shell

root@node-0:~# kubectl -n kube-system rollout restart deployment coredns 
```

#

```shell
root@node-0:~# kubectl -n wrk-1 get pods
NAME                                   READY   STATUS    RESTARTS   AGE
node-wrk-1-ubuntu24-684f7d8fd6-2zncq   1/1     Running   0          13m
root@node-0:~# kubectl -n wrk-1 exec -it node-wrk-1-ubuntu24-684f7d8fd6-2zncq -- /bin/bash
root@node-wrk-1-ubuntu24-684f7d8fd6-2zncq:/workspace# 

```


```shell
root@node-0:~# kubectl -n wrk-2 get pods
NAME                                   READY   STATUS    RESTARTS   AGE
node-wrk-2-ubuntu24-85748464f7-mwmrt   1/1     Running   0          11m
root@node-0:~# kubectl -n wrk-2 exec -it node-wrk-2-ubuntu24-85748464f7-mwmrt -- /bin/bash
root@node-wrk-2-ubuntu24-85748464f7-mwmrt:/workspace# 

```

#

```shell
root@node-wrk-2-ubuntu24-85748464f7-mwmrt:/workspace# nc -l 0.0.0.0 9999
```

```shell
root@node-wrk-1-ubuntu24-684f7d8fd6-2zncq:/workspace# nc node-wrk-2-ubuntu24.wrk-2 9999
Ncat: Could not resolve hostname "node-wrk-2-ubuntu24.wrk-2": Temporary failure in name resolution. QUITTING.

```

#

```shell

root@node-wrk-1-ubuntu24-684f7d8fd6-2zncq:/workspace# nc 10.168.0.2 9999
asdfdas

seantywork@node-0:~$ nc -l 10.168.0.2 9999
asdfdas


```

#

```shell

root@node-1:~# ip a
...
5: cilium_vxlan: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1460 qdisc noqueue state UNKNOWN group default 
    link/ether b2:32:b8:a8:45:44 brd ff:ff:ff:ff:ff:ff
    inet6 fe80::b032:b8ff:fea8:4544/64 scope link 
       valid_lft forever preferred_lft forever
...
```

#

```shell

root@node-1:~# tshark -i cilium_vxlan -f tcp
Running as user "root" and group "root". This could be dangerous.
Capturing on 'cilium_vxlan'
    1 0.000000000    10.0.1.49 → 10.0.0.252   HTTP 167 GET /hello HTTP/1.1 
    2 0.032000570   10.0.1.145 → 10.0.0.31    TCP 66 4240 → 46148 [ACK] Seq=1 Ack=1 Win=509 Len=0 TSval=371927877 TSecr=1863786270
    3 0.207922950    10.0.1.49 → 10.0.0.252   TCP 167 [TCP Retransmission] 55212 → 4240 [PSH, ACK] Seq=1 Ack=1 Win=504 Len=101 TSval=1496745839 TSecr=99162087
    4 0.415906275    10.0.1.49 → 10.0.0.252   TCP 167 [TCP Retransmission] 55212 → 4240 [PSH, ACK] Seq=1 Ack=1 Win=504 Len=101 TSval=1496746047 TSecr=99162087
    5 0.823897888    10.0.1.49 → 10.0.0.252   TCP 167 [TCP Retransmission] 55212 → 4240 [PSH, ACK] Seq=1 Ack=1 Win=504 Len=101 TSval=1496746455 TSecr=99162087
    6 1.631940972    10.0.1.49 → 10.0.0.252   TCP 167 [TCP Retransmission] 55212 → 4240 [PSH, ACK] Seq=1 Ack=1 Win=504 Len=101 TSval=1496747263 TSecr=99162087
```


#

```shell


root@node-1:~# tshark -i cilium_vxlan -f "tcp port 9999"
Running as user "root" and group "root". This could be dangerous.
Capturing on 'cilium_vxlan'
    1 0.000000000    10.0.1.15 → 10.0.2.215   TCP 74 55470 → 9999 [SYN] Seq=0 Win=64390 Len=0 MSS=1370 SACK_PERM TSval=123429871 TSecr=0 WS=128
    2 0.000493751   10.0.2.215 → 10.0.1.15    TCP 74 9999 → 55470 [SYN, ACK] Seq=0 Ack=1 Win=65184 Len=0 MSS=1370 SACK_PERM TSval=2041106803 TSecr=123429871 WS=128
    3 0.000602455    10.0.1.15 → 10.0.2.215   TCP 66 55470 → 9999 [ACK] Seq=1 Ack=1 Win=64512 Len=0 TSval=123429872 TSecr=2041106803

```

#

```shell

root@node-wrk-2-ubuntu24-85748464f7-mwmrt:/workspace# nc -l 0.0.0.0 9999
asdfasdfasdfasd
```

```shell
root@node-wrk-1-ubuntu24-684f7d8fd6-2zncq:/workspace# nc node-wrk-2-ubuntu24.wrk-2 9999
asdfasdfasdfasd

```